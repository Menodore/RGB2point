{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c63a32fe-fc7a-4e62-b348-27c0510704ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch torchvision timm accelerate open3d scikit-learn chamferdist\n",
    "# !pip install timm accelerate open3d scikit-learn chamferdist  opencv-python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "66ae18b7-ecdc-459a-b93a-a985086e8d02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running on cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7ee59121c170>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import random\n",
    "import numpy as np\n",
    "import cv2\n",
    "import gc\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "import timm\n",
    "\n",
    "\n",
    "DATA_ROOT = Path(\"/dataset/\") \n",
    "SAVE_DIR  = Path(\"./runs_rgb2point_output\")\n",
    "SAVE_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "\n",
    "FRAMES_PER_SCENE = 10        # how many images we keep per scene\n",
    "POINTS_N = 512               # cloud size during training\n",
    "BATCH = 5                    # batch size\n",
    "EPOCHS = 10                 \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available()\n",
    "                      else \"mps\" if torch.backends.mps.is_available()\n",
    "                      else \"cpu\")\n",
    "print(\"running on\", device)\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b6daaca5-febf-4926-a14a-27f0574a92ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 train scenes\n",
      "11 val scenes\n",
      "12 test scenes\n"
     ]
    }
   ],
   "source": [
    "#all pid\n",
    "all_pids = sorted(p.name for p in DATA_ROOT.iterdir() if p.is_dir())\n",
    "random.shuffle(all_pids)\n",
    "\n",
    "n = len(all_pids)\n",
    "train_pids = all_pids[: int(0.8 * n)]\n",
    "val_pids   = all_pids[int(0.8 * n) : int(0.9 * n)]\n",
    "test_pids  = all_pids[int(0.9 * n) :]\n",
    "\n",
    "print(len(train_pids), \"train scenes\")\n",
    "print(len(val_pids),   \"val scenes\")\n",
    "print(len(test_pids),  \"test scenes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "e32d3402-58ae-41c1-927a-99aa646c1029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pfm(path):\n",
    "    with open(path, \"rb\") as f:\n",
    "        header = f.readline().decode().strip()\n",
    "        assert header in (\"Pf\", \"PF\")\n",
    "        width, height = map(int, f.readline().split())\n",
    "        scale = float(f.readline())\n",
    "        data = np.fromfile(f, \"<f\" if scale < 0 else \">f\")\n",
    "        return data.reshape(height, width)\n",
    "\n",
    "def cam_txt_to_mats(path):\n",
    "    tokens = open(path).read().split()\n",
    "    extr = np.array(tokens[1:17], float).reshape(4, 4)\n",
    "    intr = np.array(tokens[18:27], float).reshape(3, 3)\n",
    "    return intr, extr\n",
    "\n",
    "def depth_to_xyz_sparse(depth, intr, extr, k):\n",
    "    h, w = depth.shape\n",
    "    ys = np.random.randint(0, h, k)\n",
    "    xs = np.random.randint(0, w, k)\n",
    "\n",
    "    z = depth[ys, xs]\n",
    "    x = (xs - intr[0, 2]) * z / intr[0, 0]\n",
    "    y = (ys - intr[1, 2]) * z / intr[1, 1]\n",
    "\n",
    "    cam = np.stack([x, y, z, np.ones_like(z)], -1)  # (k, 4)\n",
    "    world = (extr @ cam.T).T[:, :3]\n",
    "    return world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058289c-2cad-4181-8a1e-13b86125b38a",
   "metadata": {},
   "source": [
    "The network is a **two‑stage Transformer**:\n",
    "\n",
    "1. **ViT‑Base backbone** (16 × 16 patches, 12 heads, 12 layers) turns a\n",
    "      224 × 224 RGB image into a single 768‑D *CLS* token that encodes global\n",
    "      appearance.\n",
    "      The ViT weights are frozen so the model can be trained on a laptop in minutes.\n",
    "\n",
    "2. A lightweight **point‑cloud head**:\n",
    "      \\* a 1‑layer multi‑head self‑attention (4 heads, 1024 D) that lets the CLS\n",
    "        token reason about a learnable “point template”;\n",
    "      \\* a 2‑layer MLP that maps the attended features to\n",
    "        `N × 3` coordinates (we use **N = 256** or **512**).\n",
    "\n",
    "\n",
    " **RGB2Point** (Lee & Benes, *WACV 2025*) \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4781709-91f5-4fc0-b921-622337b0ee5b",
   "metadata": {},
   "source": [
    "# # Data Loader for BlendedMVS Dataset\n",
    "The BlendedMVS Dataset is a comprehensive multi-view stereo dataset designed for benchmarking dense 3D reconstruction algorithms. It provides high-quality, large-scale 3D models with multi-view images, camera parameters, and ground-truth depth ma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5b4c328b-916b-4c2f-9a1d-b3176c2626aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlendedSubset(Dataset):\n",
    "    def __init__(self, scene_list, frames_per_scene, sample_n):\n",
    "        self.frames = []\n",
    "        for pid in scene_list:\n",
    "            imgs = [j for j in (DATA_ROOT / pid / \"blended_images\").glob(\"*.jpg\")\n",
    "                    if \"_masked\" not in j.stem]\n",
    "            imgs = imgs[: frames_per_scene]\n",
    "            for j in imgs:\n",
    "                self.frames.append((pid, j.stem))\n",
    "\n",
    "        self.sample_n = sample_n\n",
    "        self.tfm = transforms.Compose([\n",
    "            transforms.ToPILImage(),\n",
    "            transforms.Resize((224, 224)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.485, 0.456, 0.406),\n",
    "                                 (0.229, 0.224, 0.225))\n",
    "        ])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.frames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pid, fid = self.frames[idx]\n",
    "        base = DATA_ROOT / pid\n",
    "\n",
    "        img_path = base / \"blended_images\" / f\"{fid}.jpg\"\n",
    "        img = cv2.imread(str(img_path))[..., ::-1]\n",
    "\n",
    "        depth_path = base / \"rendered_depth_maps\" / f\"{fid}.pfm\"\n",
    "        depth = read_pfm(depth_path)\n",
    "\n",
    "        intr, extr = cam_txt_to_mats(base / \"cams\" / f\"{fid}_cam.txt\")\n",
    "        xyz = depth_to_xyz_sparse(depth, intr, extr, self.sample_n)\n",
    "\n",
    "        img_t = self.tfm(img).unsqueeze(0)\n",
    "        xyz_t = torch.from_numpy(xyz).float()\n",
    "        return img_t, xyz_t\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "2469c09c-e029-4068-9114-7a212a598655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "900 train frames\n",
      "110 val frames\n"
     ]
    }
   ],
   "source": [
    "train_set = BlendedSubset(train_pids, FRAMES_PER_SCENE, POINTS_N)\n",
    "val_set   = BlendedSubset(val_pids,   FRAMES_PER_SCENE, POINTS_N * 2)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=BATCH,\n",
    "    shuffle=True,\n",
    "    num_workers=0,\n",
    "    drop_last=True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_set,\n",
    "    batch_size=1,\n",
    "    shuffle=False,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "print(len(train_set), \"train frames\")\n",
    "print(len(val_set),   \"val frames\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "d8a85ad0-ab8c-4b78-8e88-10483e9a3c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudHead(nn.Module):\n",
    "    def __init__(self, feat_dim, pc_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(\n",
    "            embed_dim=feat_dim,\n",
    "            num_heads=4,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(feat_dim, 2048),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(2048, pc_size * 3)\n",
    "        )\n",
    "        self.pc_size = pc_size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.attn(x, x, x)\n",
    "        x = self.mlp(x.flatten(start_dim=1))\n",
    "        return x.view(-1, self.pc_size, 3)\n",
    "\n",
    "class PointCloudNet(nn.Module):\n",
    "    def __init__(self, pc_size):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(\n",
    "            \"vit_base_patch16_224\",\n",
    "            pretrained=True,\n",
    "            num_classes=0\n",
    "        )\n",
    "        for p in self.backbone.parameters():\n",
    "            p.requires_grad = False\n",
    "\n",
    "        self.reduce = nn.Linear(self.backbone.num_features, 1024)\n",
    "        self.head = PointCloudHead(1024, pc_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, v, c, h, w = x.shape\n",
    "        feats = self.backbone(x.view(b * v, c, h, w))\n",
    "        feats = feats.view(b, v, -1).mean(dim=1)\n",
    "        feats = self.reduce(feats).unsqueeze(1)\n",
    "        return self.head(feats)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "36db5b7c-42b4-4ab2-b982-35e1f26b0708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chamfer_cdist(p, q):\n",
    "    d = torch.cdist(p, q)                           \n",
    "    return (d.min(2)[0].mean(1) + d.min(1)[0].mean(1)).mean()\n",
    "    \n",
    "def normalise(pc):\n",
    "    centre = pc.mean(dim=1, keepdim=True)\n",
    "    pc = pc - centre\n",
    "    scale = pc.abs().max(dim=1, keepdim=True)[0]\n",
    "    return pc / (scale + 1e-8)\n",
    "    \n",
    "def fscore(p, q, tau=0.01):\n",
    "    d = torch.cdist(p, q)\n",
    "    prec = (d.min(2)[0] < tau).float().mean(1)\n",
    "    rec  = (d.min(1)[0] < tau).float().mean(1)\n",
    "    f = 2 * prec * rec / (prec + rec + 1e-8)\n",
    "    return f.mean().item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "375285f6-949c-4711-a7df-ec028cbb0a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1  |  train Chamfer 223.4820\n",
      "val Chamfer 101.0465   F‑score 0.006\n",
      "new best model saved\n",
      "Epoch 2  |  train Chamfer 204.4432\n",
      "val Chamfer 105.8543   F‑score 0.007\n",
      "Epoch 3  |  train Chamfer 196.5042\n",
      "val Chamfer 116.7266   F‑score 0.005\n",
      "Epoch 4  |  train Chamfer 189.0235\n",
      "val Chamfer 140.2498   F‑score 0.006\n",
      "Epoch 5  |  train Chamfer 166.3376\n",
      "val Chamfer 115.2909   F‑score 0.006\n",
      "Epoch 6  |  train Chamfer 164.4245\n",
      "val Chamfer 98.2706   F‑score 0.006\n",
      "new best model saved\n",
      "Epoch 7  |  train Chamfer 165.9970\n",
      "val Chamfer 122.4682   F‑score 0.007\n",
      "Epoch 8  |  train Chamfer 156.8859\n",
      "val Chamfer 128.7129   F‑score 0.006\n",
      "Epoch 9  |  train Chamfer 161.4080\n",
      "val Chamfer 106.0725   F‑score 0.006\n",
      "Epoch 10  |  train Chamfer 154.7619\n",
      "val Chamfer 98.8186   F‑score 0.006\n",
      "Training finished. Best validation Chamfer: 98.27057601755315\n"
     ]
    }
   ],
   "source": [
    "model = PointCloudNet(pc_size=POINTS_N).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=5e-4)\n",
    "\n",
    "best_val_cd = float(\"inf\")\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    model.train()\n",
    "    epoch_losses = []\n",
    "\n",
    "    for img, gt in train_loader:\n",
    "        img = img.to(device)\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        pred = model(img)\n",
    "     \n",
    "        loss = chamfer_cdist(pred,gt)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_losses.append(loss.item())\n",
    "\n",
    "    train_cd = np.mean(epoch_losses)\n",
    "    print(f\"Epoch {epoch}  |  train Chamfer {train_cd:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    cds = []\n",
    "    fscores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for img, gt in val_loader:\n",
    "            img = img.to(device)\n",
    "            gt = gt.to(device)\n",
    "\n",
    "            pred = model(img)\n",
    "            cd_val = chamfer_cdist(pred, gt).item()\n",
    "            cds.append(cd_val)\n",
    "\n",
    "            fs_val = fscore(normalise(pred),normalise( gt))\n",
    "            fscores.append(fs_val)\n",
    "\n",
    "    val_cd = np.mean(cds)\n",
    "    val_f = np.mean(fscores)\n",
    "    print(f\"val Chamfer {val_cd:.4f}   F‑score {val_f:.3f}\")\n",
    "\n",
    "    if val_cd < best_val_cd:\n",
    "        best_val_cd = val_cd\n",
    "        torch.save(model.state_dict(), SAVE_DIR / \"best_rgb2point.pth\")\n",
    "        print(\"new best model saved\")\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "print(\"Training finished. Best validation Chamfer:\", best_val_cd)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "b6da5054-d9b0-494a-913d-790e8b1e2c70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Chamfer distance: 55.20543807347615\n",
      "Test F‑score: 0.002478624888074895\n"
     ]
    }
   ],
   "source": [
    "test_set = BlendedSubset(test_pids, FRAMES_PER_SCENE, POINTS_N * 2)\n",
    "test_loader = DataLoader(test_set, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "model.load_state_dict(torch.load(SAVE_DIR / \"best_rgb2point.pth\"))\n",
    "model.eval()\n",
    "\n",
    "cds = []\n",
    "fscores = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for img, gt in test_loader:\n",
    "        img = img.to(device)\n",
    "        gt = gt.to(device)\n",
    "\n",
    "        pred = model(img)\n",
    "        cds.append(chamfer_cdist(pred, gt).item())\n",
    "        fscores.append(fscore(normalise(pred), normalise(gt)))\n",
    "\n",
    "print(\"Test Chamfer distance:\", np.mean(cds))\n",
    "print(\"Test F‑score:\", np.mean(fscores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8983a74a-f02d-435e-b739-8706bfa1ad25",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "c8fe0888-261b-441e-8dbf-b11f92baffed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "81ff70f2-1ecf-4497-8c98-006cec8e90e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea56587-5211-49bb-8d49-1523689c2d9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f04fd8-5453-4713-b612-35eb24feac7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
